# 3. Random Variable. Expectation. Independence

## 3.1. General Definitions


1. f

2. d

3. Define r.v. X as the identity, so $X(B)=B$ for $B\in\mathscr B_1$. Then $\mu(X^{-1}(B))=\mu(B)$. An identity mapping may be not continuous in some spaces, which means there exists an open set $B$ such that $X^{-1}(B)$ is not open, thus $\mu\circ X^{-1}$ is not defined at $B$.

4. We need to show $\mathscr P\{G(\theta)\le x\}=F(x)$ for all $x\in\R$. This is equivalent to $\mathscr P\{\sup\{y;F(y)\le\theta(\omega)\}\le x\}=F(x)$ for all $x$. $\sup\{y;F(y)\le\theta\}\le x \iff F(x)\ge \theta$, and $\mathscr P\{F(x)\ge\theta(\omega)\}=P\{\theta(\omega)\le F(x)\}=D(F(x))=F(x)$ where $D$ is a d.f. of uniform distribution on $[0,1]$. Hence it is proved.

5. Let $G$ the d.f. of of $F(X)$. Then for $k\in[0,1]$, $G(k)=\mathscr P\{F(x)\le k\}=\mathscr P\{x\le F^{-1}(k)\}=F(F^{-1}(k))=k$ where $x\le F^{-1}(k)$ means $x$ is equal to or smaller than any element of $F^{-1}(\{k\})$. Hence $F(X)$ has the uniform distribution on $[0,1]$. The continuity is used for $F^{-1}(k)\neq\varnothing$. If $F$ is not continuous, for example $F$ jumps from $0.2$ to $0.3$ at $x_0$, $G(k)$ is not defined on $(0.2,0.3)$. Clearly this is not uniform distribution.

6..

7..

8..

9..

10..

11. ($X$ is r.v.) Since $X$ is measurable, $X^{-1}(B)\in \mathscr F\{X\}$. Conversely, $\mathscr F\{X\}$ is generated by a collection of $X^{-1}(B)$ for $B\in\mathscr B^1$. Any element of $\mathscr F\{X\}$ is made of complement/union/intersection of basis elements, and the inverse is commutative with complement/union/intersection, hence $\Lambda\in\mathscr F\{X\}\Longrightarrow \Lambda=X^
{-1}(B)$ for some $B$.  $B$ is not unique if $x$ is not surjective.

## 3.2. Properties of mathematical expectation

1..

2. (The statement is wrong in a case of direc-delta function?) [WIP] In particular, suppose $\lim_{n\Longrightarrow\infty}\mathscr P(|X|>n)=k>0$. Then it is a contradiction that $\int_X Xd\mathscr P\ge nk$ for all $n$ and thus $\int_X Xd\mathscr P=\infty$. Therefore $\lim_{n\Longrightarrow\infty}\mathscr P(|X|>n)=0$. (Or, more easily, use Theorem 3.2.1) Using the first statement in the problem,  $\lim_{n\longrightarrow\infty}\int_{|X|>n}Xd\mathscr P=0$.

3..

4..

5..

6..

7..

8..

9..

10..

11..

12..

13..

14..

15..

16.(*) [WIP] (if F is strictly increasing between 0 and 1) $\int_{-\infty}^{\infty}[F(x+a)-F(x)]dx=\int_0^1 m(F_a^{-1}(x)\setminus F^{-1}(x)) dy = a$. (Use lebesgue?)

17..

18..
19..
20..

## 3.3. Independence

1. [WIP] Let $\mathscr F=\{\varnothing,\{a\},\{b\},\{a,b\}\}, \mathscr P_1(\{a\})=0.5, \mathscr P_1(\{b\})=0.5, \mathscr P_1(\{a,b\})=1, X_1(a)=1, X_1(b)=2, X_2(a)=1, X_2(b)=1$. Then $\mathscr P(X_1=1)=0.5, P(X_2=1)=1$

2. It can be easily checked that they are pairwise independent. They are not totally independent because $P(X_1X_2=1,X_1=1,X_2=1)=1/4\neq P(X_1=1)P(X_1=1)P(X_1X_2=1)=1/8$. $\mathscr F=\{X_1,X_2,\dots,X_{n-1},X_1X_2X_3\dots X_{n-1}\}$ is an example that every $n-1$ of them are independent but not all of them. Let $\Lambda_1=\{b,c\},\Lambda_2=\{a,b\},\Lambda_3=\{c,d\}, P(b)=1/4=P(ab)P(bc)=1/2*1/2$ and $P(c)=1/4=P(bc)P(cd)=1/2*1/2$. Then $\Lambda_1$ and $\Lambda_2\cup\Lambda_3=\{a,b,c,d\}$ are not independent.

3..

4..

5..

6..

7..

8..

9. $\int_{Y\in B}Xd\mathscr P=\int X\Delta_B(Y)d\mathscr P=\mathscr E(X\Delta_B(Y))=\mathscr E(X) \mathscr E(\Delta_B(Y))=\mathscr E(X) \mathscr P(Y\in B)$. $X$ and $\Delta_B(Y)$ are independent because the identity is measurable and an indicator of a borel set is measurable, so by Theorem 3.3.1 they are independent.

10. Let any possible value of $Y$ be $y$. By Fubini, $\mathscr E(|X+y|^p)<\infty$. There exists a constant $c\in(0,\infty)$ such that $|a+b|^p\le c(|a|^p+|b|^p)$ for all $a, b$.  Then $\mathscr E(|X|^p)=E(|X+y-y|^p)\le \mathscr E((|X+y|+|y|)^p)\le\mathscr E(c(|X+y|^p+|y|^p))=\mathscr cE(|X+y|^p)+|y|^p<\infty$. The similar proof for $\mathscr E(Y)$ holds. [link](https://math.stackexchange.com/questions/1589248/if-the-sum-of-two-independent-random-variables-is-lp-does-it-imply-that)

